---
# Deploy the Spark infraestructure on every cluster node.
# Target hosts: tdm_master:tdm_worker:local
#

- name: Check availability of VM's
  copy:
    content: "  VM gestionada por los roles/playbooks de Ansible.\n"
    dest: /etc/motd
  become: true

- name: Include the YUM update role
  include_role:
    name: packages
  vars:
    packages:
      - regex_name: ncdu
      - regex_name: epel-release
      - regex_name: python38
      - regex_name: java
      - regex_name: java-devel
  when: ( group_names | select('match', 'local') | list | length ) == 0

- name: Deploy the Java profile.d shell script
  block:
    - name: Create an empty Java profile.d file
      file:
        path: /etc/profile.d/java.sh
        owner: root
        group: root
        mode: '0644'
        state: touch

    - name: Fill the Java profile.d file
      copy:
        dest: /etc/profile.d/java.sh
        content: |
          export JAVA_HOME=/etc/alternatives/jre
          export PATH=${JAVA_HOME}/bin:${PATH}

  become: true
  when: ( group_names | select('match', 'local') | list | length ) == 0

- name: Include the Spark user creation role
  include_role:
    name: user_creation
  vars:
    username: 'spark'
  when: ( group_names | select('match', 'local') | list | length ) == 0

- name: Install pyspark library
  pip:
    name: pyspark
  when: ( group_names | select('match', 'local') | list | length ) == 0

- name: Download the latest Spark tarball and set its name as a variable into the role vars file
  block:
    - name: Download the latest Spark tarball
      shell: |
        spark_version=$(wget -qO- https://archive.apache.org/dist/spark/ | grep 'spark-[0-9]\.*' | grep -iv '\-[a-z]' | sed -e 's/.*href=//' | cut -d'"' -f2 | sort | tail -1)
        file=$(wget -qO- https://archive.apache.org/dist/spark/${spark_version} | grep -i 'bin-hadoop[1-9\.*]*.tgz"' | sed -e 's/.*href=//' | cut -d'"' -f2 | sort | tail -1)
        wget -qO /home/ansible/roles/spark/files/${file} https://archive.apache.org/dist/spark/${spark_version}${file}
        sed -i "s/tarball.*$/tarball: ${file}/" /home/ansible/roles/spark/vars/main.yml
      ignore_errors: yes

    - name: Reload vars file
      include_vars:
        dir: /home/ansible/roles/spark/vars

  when: ( group_names | select('match', 'local') | list | length ) > 0

- name: Copy the latest Spark tarball into every node
  copy:
    src: "{{ tarball }}"
    dest: /home/spark
    owner: spark
    group: spark
    mode: '0644'
  when: ( group_names | select('match', 'local') | list | length ) == 0
 
...
