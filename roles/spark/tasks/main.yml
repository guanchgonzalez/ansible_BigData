---
# Deploy the Spark infraestructure on every cluster node.
# Target hosts: tdm_master:tdm_worker:local
#

- name: Include common deployment tasks for Spark infraestructure
  include_tasks: "{{ playbook_dir }}/common_tasks/main.yml"

- name: Install pyspark library
  become: true
  when: inventory_hostname != 'localhost'
  pip:
    name: pyspark

- name: Create the Spark directories and the env file for every node
  become: true
  become_user: "{{ username }}"
  when: inventory_hostname != 'localhost'
  block:
  - name: Reload vars file
    include_vars: "{{ role_path }}/vars/main.yml"

  - name: Create the local subdirectories
    loop:
      - 'data'
      - 'logs'
      - 'pid'
      - 'worker'
    file:
      path: "/home/{{ username }}/{{ version }}/local/{{ item }}"
      mode: '0755'
      state: directory

  - name: Get Memory (in MB) and virtual CPU facts
    loop:
      - 'ansible_*_mb'
      - 'ansible_processor_*'
    setup:
      filter: "{{ item }}"

  - name: Show all collected facts
    debug:
      var: ansible_facts

  - name: Create a copy of the spark-env.sh template
    copy:
      src: "/home/{{ username }}/{{ version }}/conf/spark-env.sh.template"
      dest: "/home/{{ username }}/{{ version }}/conf/spark-env.sh"
      remote_src: yes

  - name: Fill the spark-env.sh file
    blockinfile:
      path: "/home/{{ username }}/{{ version }}/conf/spark-env.sh"
      mode: '0755'
      marker: "# {mark} ENV SPARK BLOCK"
      block: |
        export SPARK_LOCAL_HOSTNAME={{ inventory_hostname }}
        export SPARK_MASTER_HOST={{ groups.tdm_master | first }}
        export SPARK_WORKER_INSTANCES=1
        export SPARK_WORKER_CORES={{ 1 if ( ansible_processor_vcpus < 3 ) else ( ( ansible_processor_vcpus / 2 ) | int ) }}
        export SPARK_WORKER_MEMORY={{ ( ansible_memtotal_mb / 2 ) | int }}m
        export SPARK_CONF_DIR=${SPARK_HOME}/conf
        export SPARK_WORKER_DIR=${SPARK_HOME}/local/worker
        export SPARK_LOCAL_DIRS=${SPARK_HOME}/local/data
        export SPARK_LOG_DIR=${SPARK_HOME}/local/logs
        export SPARK_PID_DIR=${SPARK_HOME}/local/pid
        export PYSPARK_PYTHON=`which python3`
        export PYSPARK_DRIVER_PYTHON=${PYSPARK_PYTHON}
        export PYTHONPATH=${SPARK_HOME}/python:${PYTHONPATH}
        export PYTHONPATH=${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${PYTHONPATH}
        export PYTHONPATH=${SPARK_HOME}/python/lib/pyspark.zip:${PYTHONPATH}
        export PATH=${SPARK_HOME}/python:${PATH}

- name: Spark master tasks
  become: true
  become_user: "{{ username }}"
  when: inventory_hostname in groups['tdm_master']
  block:
  - name: Create a copy of the spark-defaults.conf template
    copy:
      src: "/home/{{ username }}/{{ version }}/conf/spark-defaults.conf.template"
      dest: "/home/{{ username }}/{{ version }}/conf/spark-defaults.conf"
      remote_src: yes

  - name: Fill the spark-defaults.conf file
    blockinfile:
      path: "/home/{{ username }}/{{ version }}/conf/spark-defaults.conf"
      mode: '0755'
      marker: "# {mark} SPARK CONF BLOCK"
      block: |
        spark.ui.port 8082
        spark.master spark://0.0.0.0:7077
        spark.driver.cores {{ 1 if ( ansible_processor_vcpus < 3 ) else ( ( ansible_processor_vcpus / 2 ) | int ) }}
        spark.driver.memory {{ ( ansible_memtotal_mb / 2 ) | int }}m
        spark.driver.maxResultSize 0
        spark.executor.cores {{ 1 if ( ansible_processor_vcpus < 3 ) else ( ( ansible_processor_vcpus / 4 ) | int ) }}
        spark.executor.memory {{ ( ansible_memtotal_mb / 4 ) | int }}m
        spark.serializer org.apache.spark.serializer.KryoSerializer
        spark.rdd.compress true
        spark.kryoserializer.buffer.max 128m
        spark.blockManager.port 38000
        spark.broadcast.port 38001
        spark.driver.port 38002
        spark.executor.port 38003
        spark.fileserver.port 38004
        spark.replClassServer.port 38005

  - name: Create a copy of the slaves template
    copy:
      src: "/home/{{ username }}/{{ version }}/conf/workers.template"
      dest: "/home/{{ username }}/{{ version }}/conf/slaves"
      remote_src: yes

  - name: Remove localhost line from the slaves file
    lineinfile:
      path: "/home/{{ username }}/{{ version }}/conf/slaves"
      regexp: '^localhost'
      state: absent

  - name: Fill the slaves file
    loop: "{{ groups.tdm_worker }}"
    lineinfile:
      path: "/home/{{ username }}/{{ version }}/conf/slaves"
      line: "{{ item }}"
      mode: '0644'

  - name: Deploy the Python scripts
    loop:
      - 'TDM_P2_anexo_1_Pi.py'
      - 'TDM_P2_anexo_2_WordCount.py'
    copy:
      src: "{{ item }}"
      dest: "/home/{{ username }}/{{ version }}/examples/src/main/python/"
      owner: "{{ username }}"
      group: bigdata
      mode: '0755'

  - name: Start the Spark cluster
    command: "/home/{{ username }}/{{ version }}/sbin/start-all.sh"

...
